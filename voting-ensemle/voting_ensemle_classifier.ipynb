{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":420,"sourceType":"datasetVersion","datasetId":19}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Alright, let‚Äôs go deep into the **Voting Classifier** in machine learning.\n\n---\n\n# üîπ What is a Voting Classifier?\n\nA **Voting Classifier** is an **ensemble learning** technique that combines predictions from multiple different machine learning models (called *base learners* or *estimators*) to make a final decision.\n\nThe idea is:\n\n* Instead of relying on a single model, combine several models to improve generalization and reduce the risk of overfitting.\n* Each model contributes its ‚Äúvote‚Äù to the final prediction.\n\nIt is implemented in **`sklearn.ensemble.VotingClassifier`**.\n\n---\n\n# üîπ Types of Voting\n\n## 1. **Hard Voting (Majority Voting)**\n\n* Each classifier predicts a **class label**.\n* The final prediction is the class that receives the **majority of votes**.\n* Example:\n\n  * Model A ‚Üí Class 0\n  * Model B ‚Üí Class 1\n  * Model C ‚Üí Class 1\n  * Final prediction ‚Üí **Class 1** (majority wins).\n\n**Use cases**:\n\n* Works well when base classifiers are accurate and diverse.\n* Easy to interpret.\n* Good for discrete classification problems.\n\n---\n\n## 2. **Soft Voting (Weighted Probability Averaging)**\n\n* Each classifier predicts a **probability distribution** over classes.\n* The probabilities are averaged (optionally weighted).\n* The class with the **highest average probability** is chosen.\n* Example:\n\n  * Model A ‚Üí \\[0.7, 0.3]\n  * Model B ‚Üí \\[0.6, 0.4]\n  * Model C ‚Üí \\[0.8, 0.2]\n  * Average ‚Üí \\[0.7, 0.3] ‚Üí Final prediction ‚Üí **Class 0**\n\n**Use cases**:\n\n* Generally performs better than hard voting.\n* Requires classifiers to support `predict_proba()`.\n* More robust when models output well-calibrated probabilities.\n\n---\n\n# üîπ Weighted Voting\n\nBoth **hard** and **soft** voting can use **weights**:\n\n* Assign higher weights to stronger models.\n* Example: `(weights=[1, 2, 3])` means the third model‚Äôs vote/probability counts 3√ó more.\n\n---\n\n# üîπ Why Use Voting Classifiers?\n\n‚úÖ **Improved accuracy** ‚Äî reduces variance & bias by combining models.\n‚úÖ **Model diversity** ‚Äî linear + tree + boosting models cover different patterns.\n‚úÖ **Robustness** ‚Äî less sensitive to errors of a single weak model.\n‚úÖ **Flexibility** ‚Äî can combine very different algorithms.\n\n---\n\n# üîπ When Voting Classifier Works Best?\n\n* When base models are **diverse** (different biases).\n\n  * Example: Logistic Regression (linear), Random Forest (tree-based), XGBoost (boosting).\n* When base models are **individually strong but not perfect**.\n* When their errors are **uncorrelated** ‚Äî combining helps cancel out mistakes.\n\n---\n\n# üîπ Limitations\n\n‚ùå If base models are very similar, voting won‚Äôt add much value.\n‚ùå Requires more computation (training multiple models).\n‚ùå Soft voting requires probability outputs (not all models support this).\n\n---\n\n# üîπ Extensions\n\n* **Weighted Voting** ‚Üí give priority to stronger models.\n* **Stacking** ‚Üí train a meta-model on predictions of base learners (more powerful).\n* **Bagging/Boosting** ‚Üí alternative ensemble approaches.\n\n---\n\nüëâ In short:\nA **Voting Classifier** is a **simple but powerful ensemble method** that combines multiple models using majority vote (hard) or probability averaging (soft). It improves performance, stability, and robustness compared to individual models.\n\n---\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/iris/Iris.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.iloc[:,1:]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Label encode Species\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = LabelEncoder()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Species'] = encoder.fit_transform(df['Species'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(df,hue='Species')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df = df[df['Species'] != 0][['SepalLengthCm','SepalWidthCm','Species']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.iloc[:,0:2]\ny = df.iloc[:,-1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clf1 = LogisticRegression()\nclf2 = RandomForestClassifier()\nclf3 = KNeighborsClassifier()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"estimators = [('lr',clf1),('rf',clf2),('knn',clf3)]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for estimator in estimators:\n    x = cross_val_score(estimator[1],X,y,cv=10,scoring='accuracy')\n    print(estimator[0],np.round(np.mean(x),2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vc = VotingClassifier(estimators=estimators,voting='hard')\nx = cross_val_score(vc,X,y,cv=10,scoring='accuracy')\nprint(np.round(np.mean(x),2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vc1 = VotingClassifier(estimators=estimators,voting='soft')\nx = cross_val_score(vc1,X,y,cv=10,scoring='accuracy')\nprint(np.round(np.mean(x),2))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(1,4):\n    for j in range(1,4):\n        for k in range(1,4):\n            vc = VotingClassifier(estimators=estimators,voting='soft',weights=[i,j,k])\n            x = cross_val_score(vc,X,y,cv=10,scoring='accuracy')\n            print(\"for i={},j={},k={}\".format(i,j,k),np.round(np.mean(x),2))","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-09-16T18:34:12.101252Z","shell.execute_reply.started":"2025-09-16T18:33:09.979051Z","shell.execute_reply":"2025-09-16T18:34:12.099845Z"}},"outputs":[{"name":"stdout","text":"for i=1,j=3,k=3 0.75\nfor i=2,j=1,k=1 0.77\nfor i=2,j=1,k=2 0.77\nfor i=2,j=1,k=3 0.77\nfor i=2,j=2,k=1 0.77\nfor i=2,j=2,k=2 0.76\nfor i=2,j=2,k=3 0.75\nfor i=2,j=3,k=1 0.74\nfor i=2,j=3,k=2 0.77\nfor i=2,j=3,k=3 0.76\nfor i=3,j=1,k=1 0.8\nfor i=3,j=1,k=2 0.78\nfor i=3,j=1,k=3 0.79\nfor i=3,j=2,k=1 0.79\nfor i=3,j=2,k=2 0.77\nfor i=3,j=2,k=3 0.77\nfor i=3,j=3,k=1 0.75\nfor i=3,j=3,k=2 0.77\nfor i=3,j=3,k=3 0.77\n","output_type":"stream"}],"execution_count":175},{"cell_type":"code","source":"from sklearn.svm import SVC","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:34:12.102128Z","iopub.execute_input":"2025-09-16T18:34:12.102468Z","iopub.status.idle":"2025-09-16T18:34:12.108926Z","shell.execute_reply.started":"2025-09-16T18:34:12.102435Z","shell.execute_reply":"2025-09-16T18:34:12.107678Z"}},"outputs":[],"execution_count":176},{"cell_type":"code","source":"from sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=2)\n\nsvm1 = SVC(probability=True, kernel='poly', degree=1)\nsvm2 = SVC(probability=True, kernel='poly', degree=2)\nsvm3 = SVC(probability=True, kernel='poly', degree=3)\nsvm4 = SVC(probability=True, kernel='poly', degree=4)\nsvm5 = SVC(probability=True, kernel='poly', degree=5)\n\nestimators = [('svm1',svm1),('svm2',svm2),('svm3',svm3),('svm4',svm4),('svm5',svm5)]\n\nfor estimator in estimators:\n    x = cross_val_score(estimator[1],X,y,cv=10,scoring='accuracy')\n    print(estimator[0],np.round(np.mean(x),2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:34:12.110073Z","iopub.execute_input":"2025-09-16T18:34:12.110468Z","iopub.status.idle":"2025-09-16T18:34:18.040697Z","shell.execute_reply.started":"2025-09-16T18:34:12.110441Z","shell.execute_reply":"2025-09-16T18:34:18.039726Z"}},"outputs":[{"name":"stdout","text":"svm1 0.85\nsvm2 0.85\nsvm3 0.89\nsvm4 0.81\nsvm5 0.86\n","output_type":"stream"}],"execution_count":177},{"cell_type":"code","source":"vc1 = VotingClassifier(estimators=estimators,voting='soft')\nx = cross_val_score(vc1,X,y,cv=10,scoring='accuracy')\nprint(np.round(np.mean(x),2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-16T18:34:18.041577Z","iopub.execute_input":"2025-09-16T18:34:18.041864Z","iopub.status.idle":"2025-09-16T18:34:23.845253Z","shell.execute_reply.started":"2025-09-16T18:34:18.041836Z","shell.execute_reply":"2025-09-16T18:34:23.844078Z"}},"outputs":[{"name":"stdout","text":"0.93\n","output_type":"stream"}],"execution_count":178}]}